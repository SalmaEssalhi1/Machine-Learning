\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{titlesec}

\geometry{margin=2.5cm}

% Configuration pour le code
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    showstringspaces=false
}

% Configuration des titres
\titleformat{\section}
{\Large\bfseries\color{blue!70!black}}
{}{0em}{}[\titlerule]

\titleformat{\subsection}
{\large\bfseries\color{blue!50!black}}
{}{0em}{}

\titleformat{\subsubsection}
{\normalsize\bfseries}
{}{0em}{}

% Métadonnées
\title{RAPPORT ATELIER 4 : Q-LEARNING PONG}
\author{Université Abdelmalek Essaadi - FST Tanger\\Machine Learning - LSI S3 - 2025/2026}
\date{\today}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Développement de la classe Agent avec les méthodes nécessaires pour l'algorithme Q-Learning}

\subsection{Structure de la classe Qlearning}

La classe \texttt{Qlearning} est implémentée dans le fichier \texttt{agent.py} et contient toutes les fonctionnalités nécessaires pour l'algorithme Q-Learning.

\subsubsection{Paramètres d'initialisation}

\begin{lstlisting}
def __init__(self, alpha=0.5, gamma=0.9, eps=0.1, eps_decay=0.):
\end{lstlisting}

\begin{itemize}
    \item \textbf{\texttt{alpha}} (taux d'apprentissage) : Contrôle la vitesse d'apprentissage ($0 < \alpha \leq 1$). Défaut : 0.5
    \item \textbf{\texttt{gamma}} (facteur de discount) : Détermine l'importance des récompenses futures ($0 \leq \gamma \leq 1$). Défaut : 0.9
    \item \textbf{\texttt{eps}} (epsilon) : Probabilité d'exploration dans la politique $\varepsilon$-greedy. Défaut : 0.1
    \item \textbf{\texttt{eps\_decay}} : Taux de décroissance d'epsilon après chaque épisode. Défaut : 0.0
\end{itemize}

\subsubsection{Attributs de la classe}

\begin{itemize}
    \item \textbf{\texttt{Q}} : Q-table stockée comme \texttt{collections.defaultdict(float)}, indexée par des tuples \texttt{(state, action)}
    \item \textbf{\texttt{actions}} : Liste des actions possibles \texttt{[-1, 0, 1]} correspondant à :
    \begin{itemize}
        \item \texttt{-1} : Déplacer la raquette vers le haut
        \item \texttt{0} : Rester immobile
        \item \texttt{1} : Déplacer la raquette vers le bas
    \end{itemize}
    \item \textbf{\texttt{rewards\_history}} : Historique des récompenses par épisode
    \item \textbf{\texttt{episode\_rewards}} : Récompense cumulée de l'épisode en cours
\end{itemize}

\subsection{Méthodes principales de l'algorithme Q-Learning}

\subsubsection{\texttt{get\_Q\_value(state, action)}}

\textbf{Rôle} : Récupère la valeur Q pour une paire état-action donnée.

\begin{lstlisting}
def get_Q_value(self, state, action):
    return self.Q[(state, action)]
\end{lstlisting}

\textbf{Fonctionnalité} : Retourne la valeur Q stockée dans la Q-table. Si la paire \texttt{(state, action)} n'existe pas, le \texttt{defaultdict} retourne automatiquement \texttt{0.0}.

\subsubsection{\texttt{get\_max\_Q(state)}}

\textbf{Rôle} : Calcule la valeur Q maximale pour un état donné sur toutes les actions possibles.

\begin{lstlisting}
def get_max_Q(self, state):
    q_values = [self.get_Q_value(state, a) for a in self.actions]
    return max(q_values)
\end{lstlisting}

\textbf{Fonctionnalité} : Utilisée dans la mise à jour Q-Learning pour calculer $\max_{a'}(Q(s', a'))$ selon l'équation de Bellman.

\subsubsection{\texttt{get\_action(state)} - Politique $\varepsilon$-greedy}

\textbf{Rôle} : Sélectionne une action selon la politique $\varepsilon$-greedy (exploration/exploitation).

\begin{lstlisting}
def get_action(self, state):
    if random.random() < self.eps:
        return random.choice(self.actions)  # Exploration
    else:
        # Exploitation : action avec Q-value maximale
        q_values = [self.get_Q_value(state, a) for a in self.actions]
        max_q = max(q_values)
        best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]
        return random.choice(best_actions)
\end{lstlisting}

\textbf{Fonctionnalité} :
\begin{itemize}
    \item \textbf{Exploration} (probabilité \texttt{eps}) : Choisit une action aléatoire pour découvrir de nouvelles stratégies
    \item \textbf{Exploitation} (probabilité \texttt{1-eps}) : Choisit l'action avec la Q-value maximale (politique greedy)
    \item Si plusieurs actions ont la même Q-value maximale, en choisit une aléatoirement parmi elles
\end{itemize}

\subsubsection{\texttt{update(s, s\_, a, a\_, r)} - Mise à jour Q-Learning}

\textbf{Rôle} : Met à jour la Q-table selon l'équation de Bellman pour Q-Learning.

\begin{lstlisting}
def update(self, s, s_, a, a_, r):
    current_q = self.get_Q_value(s, a)
    max_next_q = self.get_max_Q(s_)
    new_q = current_q + self.alpha * (r + self.gamma * max_next_q - current_q)
    self.Q[(s, a)] = new_q
    self.episode_rewards += r
\end{lstlisting}

\textbf{Équation de Bellman implémentée} :
\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma \cdot \max_{a'}(Q(s',a')) - Q(s,a)\right]
\end{equation}

\textbf{Fonctionnalité} :
\begin{itemize}
    \item Calcule la valeur Q actuelle pour l'état \texttt{s} et l'action \texttt{a}
    \item Calcule la valeur Q maximale du nouvel état \texttt{s\_} (Q-Learning est off-policy, donc utilise le max)
    \item Met à jour la Q-value en combinant la récompense immédiate \texttt{r} et la valeur future estimée
    \item Accumule la récompense dans \texttt{episode\_rewards} pour le suivi statistique
\end{itemize}

\textbf{Note importante} : Le paramètre \texttt{a\_} (action suivante) n'est pas utilisé car Q-Learning est un algorithme off-policy qui utilise toujours la valeur maximale du prochain état.

\subsubsection{\texttt{end\_episode()}}

\textbf{Rôle} : Finalise un épisode en sauvegardant les statistiques et en décroissant epsilon.

\begin{lstlisting}
def end_episode(self):
    self.rewards_history.append(self.episode_rewards)
    self.episode_rewards = 0
    self.eps = self.eps * (1 - self.eps_decay)
    self.eps = max(self.eps, 0.01)  # Minimum threshold
\end{lstlisting}

\textbf{Fonctionnalité} :
\begin{itemize}
    \item Sauvegarde la récompense totale de l'épisode dans \texttt{rewards\_history}
    \item Réinitialise le compteur de récompense pour le prochain épisode
    \item Décroît epsilon selon le taux \texttt{eps\_decay} pour réduire progressivement l'exploration
    \item Maintient epsilon à un minimum de 0.01 pour garantir une exploration minimale continue
\end{itemize}

\subsubsection{Méthodes utilitaires}

\begin{itemize}
    \item \textbf{\texttt{save(filepath)}} : Sauvegarde la Q-table dans un fichier pickle pour réutilisation ultérieure.
    \item \textbf{\texttt{load(filepath)}} : Charge une Q-table pré-entraînée depuis un fichier pickle.
    \item \textbf{\texttt{get\_stats()}} : Retourne un dictionnaire avec les statistiques d'entraînement :
    \begin{itemize}
        \item Nombre total d'épisodes
        \item Nombre d'états uniques explorés
        \item Récompense moyenne
        \item Epsilon actuel
        \item Taille de la Q-table
    \end{itemize}
    \item \textbf{\texttt{\_\_str\_\_()}} : Représentation textuelle de l'agent avec ses statistiques principales.
\end{itemize}

\subsection{Conclusion sur la classe Agent}

La classe \texttt{Qlearning} implémente complètement l'algorithme Q-Learning avec :
\begin{itemize}
    \item[$\checkmark$] Gestion de la Q-table (stockage et accès)
    \item[$\checkmark$] Politique $\varepsilon$-greedy pour l'exploration/exploitation
    \item[$\checkmark$] Mise à jour selon l'équation de Bellman
    \item[$\checkmark$] Décroissance adaptative d'epsilon
    \item[$\checkmark$] Sauvegarde/chargement des modèles
    \item[$\checkmark$] Suivi des statistiques d'apprentissage
\end{itemize}

\section{Intégration de l'agent au niveau du jeu en mode Agent RL vs Agent AI}

\subsection{Architecture d'intégration}

L'intégration de l'agent Q-Learning dans le jeu Pong se fait à travers deux classes principales :
\begin{itemize}
    \item \textbf{\texttt{Game}} (dans \texttt{game.py}) : Environnement de jeu
    \item \textbf{\texttt{GameLearning}} (dans \texttt{main.py}) : Orchestrateur de l'entraînement
\end{itemize}

\subsection{Représentation de l'état}

\subsubsection{Méthode \texttt{getStateKey()}}

La méthode \texttt{getStateKey()} dans la classe \texttt{Game} discrétise l'état du jeu pour réduire l'espace d'états :

\begin{lstlisting}
def getStateKey(self):
    ball_x = int(self.ball.centerx // 80)  # 16 zones horizontales
    ball_y = int(self.ball.centery // 82)  # 10 zones verticales
    ball_vx = 1 if self.ball_speed_x > 0 else -1  # Direction horizontale
    ball_vy = 1 if self.ball_speed_y > 0 else 0 if self.ball_speed_y == 0 else -1
    player_y = int(self.player.centery // 82)  # Position raquette
    rel_y = 1 if self.ball.centery > self.player.centery else -1 if ... else 0
    return f"{ball_x}_{ball_y}_{ball_vx}_{ball_vy}_{player_y}_{rel_y}"
\end{lstlisting}

\textbf{Composantes de l'état} :
\begin{itemize}
    \item Position de la balle (x, y) discrétisée en grille
    \item Vitesse de la balle (direction x, y)
    \item Position de la raquette du joueur (y)
    \item Position relative de la balle par rapport à la raquette
\end{itemize}

Cette discrétisation permet de réduire considérablement l'espace d'états tout en conservant les informations essentielles pour la prise de décision.

\subsection{Fonction de récompense}

La fonction de récompense est définie dans \texttt{ball\_animation()} :

\begin{lstlisting}
def ball_animation(self):
    reward = 0
    # ...
    if self.ball.left <= 0:
        reward = 1  # Agent marque un point
    if self.ball.right >= self.screen_width:
        reward = -1  # Agent encaisse un point
    if self.ball.colliderect(self.player):
        reward = 0.5  # Agent touche la balle (bon comportement)
    return reward
\end{lstlisting}

\textbf{Structure des récompenses} :
\begin{itemize}
    \item \textbf{+1} : L'agent marque un point (objectif principal)
    \item \textbf{-1} : L'agent encaisse un point (pénalité)
    \item \textbf{+0.5} : L'agent touche la balle (encouragement du comportement défensif)
\end{itemize}

Cette structure de récompense guide l'agent vers l'apprentissage de stratégies défensives efficaces.

\subsection{Boucle d'entraînement RL vs AI}

La méthode \texttt{train\_rl\_vs\_ai()} dans \texttt{GameLearning} implémente la boucle d'entraînement complète :

\begin{lstlisting}
def train_rl_vs_ai(self, episodes=1000, render=False, save_interval=100):
    agent = ag.Qlearning(self.alpha, self.gamma, self.epsilon, self.eps_decay)
    game = g.Game(agent, mode='rl_vs_ai', render=render)
    
    for episode in range(episodes):
        state = game.reset()
        episode_reward = 0
        done = False
        
        while not done:
            # 1. Agent choisit une action
            action = agent.get_action(state)
            
            # 2. Exécution de l'action dans l'environnement
            new_state, reward, done = game.step(action)
            
            # 3. Mise à jour de la Q-table
            new_action = agent.get_action(new_state)
            agent.update(state, new_state, action, new_action, reward)
            
            episode_reward += reward
            state = new_state
        
        # 4. Fin d'épisode
        agent.end_episode()
\end{lstlisting}

\textbf{Étapes de la boucle} :
\begin{enumerate}
    \item \textbf{Initialisation} : Création de l'agent et du jeu en mode \texttt{rl\_vs\_ai}
    \item \textbf{Pour chaque épisode} :
    \begin{itemize}
        \item Réinitialisation du jeu (\texttt{game.reset()})
        \item \textbf{Boucle de pas} jusqu'à la fin de l'épisode :
        \begin{itemize}
            \item L'agent observe l'état actuel
            \item L'agent choisit une action via \texttt{get\_action(state)}
            \item L'environnement exécute l'action et retourne \texttt{(nouvel\_état, récompense, terminé)}
            \item L'agent met à jour sa Q-table via \texttt{update()}
        \end{itemize}
        \item Finalisation de l'épisode avec \texttt{end\_episode()}
    \end{itemize}
\end{enumerate}

\subsection{Gestion de l'adversaire AI}

L'adversaire AI est implémenté dans \texttt{opponent\_ai()} :

\begin{lstlisting}
def opponent_ai(self):
    if self.ball_speed_x < 0 and self.ball.centerx < self.ai_reaction_distance:
        if random.random() > self.ai_error_rate:
            # Suit la balle avec une certaine précision
            if self.opponent.centery < self.ball.centery - 20:
                self.opponent.y += self.opponent_speed
\end{lstlisting}

\textbf{Caractéristiques de l'AI} :
\begin{itemize}
    \item Réaction limitée : ne réagit que lorsque la balle est proche (\texttt{ai\_reaction\_distance = 400})
    \item Taux d'erreur : 15\% de chance de faire une erreur (\texttt{ai\_error\_rate = 0.15})
    \item Vitesse réduite : \texttt{opponent\_speed = 5} (plus lente que l'agent)
\end{itemize}

Ces caractéristiques rendent l'AI battable, permettant à l'agent RL d'apprendre et de progresser.

\subsection{Sauvegarde et visualisation}

\begin{itemize}
    \item \textbf{Sauvegarde automatique} : Tous les \texttt{save\_interval} épisodes (par défaut 100)
    \item \textbf{Modèle final} : Sauvegardé à \texttt{models/agent\_rl\_vs\_ai\_final.pkl}
    \item \textbf{Statistiques} : Affichées tous les 100 épisodes (récompense moyenne, taux de victoire, taille Q-table)
\end{itemize}

\section{Graphe de récompense et synthèse globale de la solution}

\subsection{Fonction de visualisation \texttt{plot\_agent\_reward()}}

La fonction \texttt{plot\_agent\_reward()} génère deux graphiques complémentaires :

\begin{lstlisting}
def plot_agent_reward(rewards, title="Agent Cumulative Reward vs. Episode", save_path=None):
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Graphique 1 : Récompense cumulative
    axes[0].plot(np.cumsum(rewards), 'b-', linewidth=1)
    axes[0].set_title('Cumulative Reward vs. Episode')
    
    # Graphique 2 : Moyenne glissante
    window = min(100, len(rewards) // 10 + 1)
    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
    axes[1].plot(moving_avg, 'g-', linewidth=1)
    axes[1].set_title(f'Moving Average Reward (window={window})')
\end{lstlisting}

\textbf{Graphiques générés} :
\begin{enumerate}
    \item \textbf{Récompense cumulative} : Montre l'évolution de la somme totale des récompenses au fil des épisodes. Une courbe ascendante indique un apprentissage progressif.
    \item \textbf{Moyenne glissante} : Affiche la tendance des performances récentes (fenêtre de 100 épisodes). Permet d'identifier les phases d'amélioration ou de stagnation.
\end{enumerate}

\subsection{Résultats typiques en mode RL vs AI}

\textbf{Comportement observé} :

\begin{itemize}
    \item \textbf{Épisodes 0-200} : 
    \begin{itemize}
        \item Exploration intensive (epsilon élevé)
        \item Récompenses très variables, souvent négatives
        \item Taux de victoire faible ($\sim$20-30\%)
        \item Q-table en croissance rapide
    \end{itemize}
    
    \item \textbf{Épisodes 200-500} :
    \begin{itemize}
        \item Début d'apprentissage de patterns
        \item Récompense moyenne qui augmente progressivement
        \item Taux de victoire en amélioration ($\sim$40-50\%)
        \item L'agent commence à suivre la balle efficacement
    \end{itemize}
    
    \item \textbf{Épisodes 500-1000} :
    \begin{itemize}
        \item Stratégie plus stable et cohérente
        \item Récompense moyenne positive
        \item Taux de victoire élevé ($\sim$60-70\%+)
        \item Exploitation dominante (epsilon faible)
    \end{itemize}
\end{itemize}

\subsection{Synthèse globale de la solution proposée}

\subsubsection{Architecture générale}

La solution propose une architecture modulaire et extensible :

\begin{center}
\begin{tabular}{|c|}
\hline
\textbf{Agent} (agent.py) \\
Q-Learning avec $\varepsilon$-greedy \\
\hline
$\downarrow$ \\
\hline
\textbf{Game} (game.py) \\
Environnement Pong \\
\hline
$\downarrow$ \\
\hline
\textbf{Main} (main.py) \\
Orchestration \& Visualisation \\
\hline
\end{tabular}
\end{center}

\subsubsection{Points forts de la solution}

\begin{enumerate}
    \item \textbf{Implémentation complète de Q-Learning} :
    \begin{itemize}
        \item Q-table efficace avec \texttt{defaultdict}
        \item Politique $\varepsilon$-greedy bien équilibrée
        \item Mise à jour correcte selon Bellman
        \item Décroissance adaptative d'epsilon
    \end{itemize}
    
    \item \textbf{Représentation d'état optimisée} :
    \begin{itemize}
        \item Discrétisation intelligente réduisant l'espace d'états
        \item Conservation des informations essentielles (position, vitesse, relation)
    \end{itemize}
    
    \item \textbf{Fonction de récompense bien conçue} :
    \begin{itemize}
        \item Récompenses claires et significatives
        \item Encouragement des comportements défensifs (+0.5 pour toucher la balle)
        \item Pénalisation des erreurs (-1 pour encaisser un point)
    \end{itemize}
    
    \item \textbf{Système de sauvegarde robuste} :
    \begin{itemize}
        \item Sauvegarde automatique périodique
        \item Chargement de modèles pré-entraînés
        \item Réutilisation entre différents modes
    \end{itemize}
    
    \item \textbf{Visualisation complète} :
    \begin{itemize}
        \item Graphiques de progression clairs
        \item Comparaison entre modes
        \item Statistiques détaillées
    \end{itemize}
\end{enumerate}

\subsubsection{Limitations et améliorations possibles}

\begin{itemize}
    \item \textbf{Espace d'états} : La discrétisation peut être améliorée pour capturer plus de nuances
    \item \textbf{Vitesse d'apprentissage} : Peut nécessiter plusieurs milliers d'épisodes pour convergence
    \item \textbf{Généralisation} : L'agent est spécialisé pour un adversaire spécifique
\end{itemize}

\textbf{Améliorations suggérées} :
\begin{itemize}
    \item Deep Q-Network (DQN) pour gérer des espaces d'états continus
    \item Experience Replay pour améliorer l'efficacité d'apprentissage
    \item Double Q-Learning pour réduire la surestimation
\end{itemize}

\section{Mode Agent RL vs Humain}

\subsection{Configuration spécifique}

En mode \texttt{rl\_vs\_human}, l'agent RL contrôle la \textbf{raquette gauche} (opponent) tandis que l'humain contrôle la \textbf{raquette droite} (player) via les flèches $\uparrow$/$\downarrow$ du clavier.

\subsection{Méthode \texttt{train\_rl\_vs\_human()}}

\begin{lstlisting}
def train_rl_vs_human(self, episodes=100, render=True):
    agent = ag.Qlearning(self.alpha, self.gamma, self.epsilon, self.eps_decay)
    
    # Chargement d'un modèle pré-entraîné si disponible
    pretrained_path = f"{self.models_dir}/agent_rl_vs_ai_final.pkl"
    if os.path.exists(pretrained_path):
        agent.load(pretrained_path)
        print("Loaded pre-trained agent!")
    
    game = g.Game(agent, mode='rl_vs_human', render=True)
\end{lstlisting}

\textbf{Caractéristiques} :
\begin{itemize}
    \item \textbf{Rendu obligatoire} : \texttt{render=True} car l'humain doit voir le jeu
    \item \textbf{Chargement de modèle} : Tente de charger un agent pré-entraîné en RL vs AI
    \item \textbf{État inversé} : Utilise \texttt{getStateKey2()} pour la perspective de l'opponent
\end{itemize}

\subsection{Gestion des entrées humaines}

Les entrées clavier sont gérées dans \texttt{step()} :

\begin{lstlisting}
if self.mode == 'rl_vs_human':
    if event.type == pygame.KEYDOWN:
        if event.key == pygame.K_DOWN:
            self.player_speed = 8
        if event.key == pygame.K_UP:
            self.player_speed = -8
\end{lstlisting}

\textbf{Contrôles} :
\begin{itemize}
    \item \textbf{Flèche $\uparrow$} : Déplacer la raquette vers le haut
    \item \textbf{Flèche $\downarrow$} : Déplacer la raquette vers le bas
\end{itemize}

\subsection{Adaptation de l'agent}

L'agent doit s'adapter au style de jeu de l'humain :

\begin{itemize}
    \item \textbf{Apprentissage adaptatif} : L'agent apprend les patterns de jeu de l'humain
    \item \textbf{Récompense inversée} : La récompense de l'agent est l'inverse de celle du joueur
    \begin{lstlisting}
    agent_reward = -reward  # Inversion pour l'agent opponent
    \end{lstlisting}
\end{itemize}

\subsection{Résultats et visualisation}

\begin{itemize}
    \item \textbf{Graphe généré} : \texttt{graphe/rewards\_rl\_vs\_human.png}
    \item \textbf{Modèle sauvegardé} : \texttt{models/agent\_rl\_vs\_human.pkl}
    \item \textbf{Statistiques} : Affichage après chaque épisode (victoire/défaite)
\end{itemize}

\textbf{Comportement observé} :
\begin{itemize}
    \item L'agent s'améliore progressivement contre le joueur humain
    \item Adaptation aux stratégies humaines (timing, angles de tir)
    \item Performance variable selon le niveau du joueur
\end{itemize}

\section{Mode Agent RL vs Agent RL}

\subsection{Architecture dual-agent}

En mode \texttt{rl\_vs\_rl}, deux agents Q-Learning indépendants s'affrontent et apprennent simultanément :

\begin{lstlisting}
def train_rl_vs_rl(self, episodes=1000, render=False, save_interval=100):
    # Création de deux agents avec paramètres légèrement différents
    agent1 = ag.Qlearning(self.alpha, self.gamma, self.epsilon, self.eps_decay)
    agent2 = ag.Qlearning(self.alpha * 0.9, self.gamma, self.epsilon * 1.1, self.eps_decay)
    
    game = g.Game(agent1, agent2=agent2, mode='rl_vs_rl', render=render)
\end{lstlisting}

\textbf{Différences entre agents} :
\begin{itemize}
    \item \textbf{Agent 1} : Paramètres standards
    \item \textbf{Agent 2} : Alpha légèrement réduit (0.9x), epsilon légèrement augmenté (1.1x)
    \item Ces différences introduisent de la diversité dans les stratégies d'apprentissage
\end{itemize}

\subsection{Boucle d'entraînement simultanée}

\begin{lstlisting}
for episode in range(episodes):
    state1 = game.reset()
    state2 = game.getStateKey2()  # État pour l'agent 2
    
    while not done:
        # Actions des deux agents
        action1 = agent1.get_action(state1)
        action2 = agent2.get_action(state2)
        
        # Exécution simultanée
        new_state1, reward, done = game.step(action1, action2)
        new_state2 = game.getStateKey2()
        
        # Récompenses inversées
        reward1 = reward      # Pour agent 1 (player)
        reward2 = -reward     # Pour agent 2 (opponent)
        
        # Mise à jour simultanée des deux Q-tables
        agent1.update(state1, new_state1, action1, new_action1, reward1)
        agent2.update(state2, new_state2, action2, new_action2, reward2)
\end{lstlisting}

\textbf{Caractéristiques} :
\begin{itemize}
    \item \textbf{Apprentissage simultané} : Les deux agents apprennent en même temps
    \item \textbf{Récompenses inversées} : Ce qui est positif pour un agent est négatif pour l'autre
    \item \textbf{États différents} : Chaque agent voit le jeu depuis sa propre perspective
\end{itemize}

\subsection{Évolution de l'apprentissage}

\begin{itemize}
    \item \textbf{Phase initiale (0-200 épisodes)} :
    \begin{itemize}
        \item Les deux agents explorent intensivement
        \item Jeux désordonnés, beaucoup d'erreurs
        \item Récompenses très variables
    \end{itemize}
    
    \item \textbf{Phase intermédiaire (200-500 épisodes)} :
    \begin{itemize}
        \item Début de stratégies cohérentes
        \item Les agents apprennent à défendre efficacement
        \item Échanges plus longs, jeux plus équilibrés
    \end{itemize}
    
    \item \textbf{Phase avancée (500-1000 épisodes)} :
    \begin{itemize}
        \item Stratégies sophistiquées émergent
        \item Les agents développent des techniques offensives et défensives
        \item Un agent peut prendre l'avantage selon les paramètres d'apprentissage
    \end{itemize}
\end{itemize}

\subsection{Visualisation et comparaison}

La fonction génère un graphique combiné avec :

\begin{enumerate}
    \item \textbf{Récompenses cumulatives} : 
    \begin{itemize}
        \item Courbe bleue pour Agent 1 (player)
        \item Courbe rouge pour Agent 2 (opponent)
        \item Permet de voir quel agent progresse le plus
    \end{itemize}
    
    \item \textbf{Taux de victoire} :
    \begin{itemize}
        \item Évolution du taux de victoire de l'Agent 1
        \item Ligne de référence à 50\% (équilibre parfait)
    \end{itemize}
\end{enumerate}

\textbf{Graphe sauvegardé} : \texttt{graphe/rewards\_rl\_vs\_rl.png}

\subsection{Modèles sauvegardés}

\begin{itemize}
    \item \textbf{Agent 1} : \texttt{models/agent1\_rl\_vs\_rl\_final.pkl}
    \item \textbf{Agent 2} : \texttt{models/agent2\_rl\_vs\_rl\_final.pkl}
    \item \textbf{Sauvegardes intermédiaires} : Tous les 100 épisodes
\end{itemize}

\subsection{Avantages du mode RL vs RL}

\begin{enumerate}
    \item \textbf{Apprentissage mutuel} : Les agents s'améliorent en s'affrontant
    \item \textbf{Diversité stratégique} : Émergence de stratégies variées
    \item \textbf{Évaluation équitable} : Comparaison directe des performances
    \item \textbf{Pas de biais humain} : Apprentissage pur sans influence humaine
\end{enumerate}

\section{Conclusion générale}

Cette implémentation de Q-Learning pour Pong démontre :

\begin{enumerate}
    \item[$\checkmark$] \textbf{Implémentation complète} de l'algorithme Q-Learning avec toutes les fonctionnalités nécessaires
    \item[$\checkmark$] \textbf{Intégration réussie} dans un environnement de jeu complexe
    \item[$\checkmark$] \textbf{Trois modes fonctionnels} permettant différents scénarios d'apprentissage
    \item[$\checkmark$] \textbf{Visualisation complète} des performances avec graphiques et statistiques
    \item[$\checkmark$] \textbf{Système robuste} de sauvegarde/chargement pour la réutilisation
\end{enumerate}

L'agent apprend progressivement à jouer au Pong en passant d'un comportement aléatoire à une stratégie cohérente et efficace, démontrant la puissance de l'apprentissage par renforcement avec Q-Learning.

\subsection{Fichiers du projet}

\begin{itemize}
    \item \texttt{agent.py} : Classe Qlearning (242 lignes)
    \item \texttt{game.py} : Classe Game avec environnement Pong (395 lignes)
    \item \texttt{main.py} : Script principal avec entraînement et visualisation (586 lignes)
    \item \texttt{models/} : Q-tables sauvegardées (.pkl)
    \item \texttt{graphe/} : Graphiques de récompenses (.png)
\end{itemize}

\end{document}

